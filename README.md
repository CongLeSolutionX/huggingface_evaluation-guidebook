# Evaluating your LLM: a guide

Links prefixed by ‚≠ê are links I enjoyed and recommend reading.

## How to read this guide
- **Beginner user**
  If you don't know anything about the topic, you should read the  `Basics` sections in each chapter, and explore the sections that interest you. 
  You'll also find explanations to support you about important LLM topics in `General knowledge`: for example, how model inference works and what tokenization is.
- **Advanced user**
  The more practical sections are the `Tips and Tricks` ones, and `Troubleshooting` chapter. 
  You can likely skip all `Basics` sections and the `General knowlege` chapter.

## Table of contents

### Intro
I'll add this section later, but if you want an intro on the topic, you can read this [blog](https://huggingface.co/blog/clefourrier/llm-evaluation) on how and why we do evaluation!
### Automatic benchmarks
- [[contents/Automated benchmarks/Basics|Basics]]
- [[Designing your automatic evaluation]]
- [[Some evaluation datasets]]
- [[contents/Automated benchmarks/Tips and tricks|Tips and tricks]]
### Human evaluation
- [[contents/Human evaluation/Basics|Basics]]
- [[Using human annotators]]
### LLM-as-a-judge
- [[contents/Model as a judge/Basics|Basics]]
- [[Getting a Judge-LLM]]
- [[Designing your evaluation prompt]]
- [[Evaluating your evaluator]]
- [[contents/Model as a judge/Tips and tricks|Tips and tricks]]
### General knowledge
These are mostly beginner guides to LLM basics, but will still contain some tips and cool references! 
If you're a advanced user, I suggest skimming to the `Going further` sections.
- [[Model inference and evaluation]]
- [[Tokenization]]

### Troubleshooting
The most densely practical part of this guide. 
- [[Troubleshooting inference]]
- [[Troubleshooting reproducibility]]

### Resources
Links I like
- [[About evaluation]]
- [[About NLP]]